name: Analysing operational characteristics for different JSON-based static data load and reload approaches

on:
  workflow_dispatch:
    inputs:
      customers:
        description: 'Number of customers to generate in the dataset'
        default: 1000000
        required: false
        type: number
      recommendations:
        description: 'Number of recommendations to generate for each customer'
        default: 20
        required: false
        type: number
      product_pool_size:
        description: 'Size of the product pool to generate in the dataset'
        default: 10000
        required: false
        type: number

jobs:
  generate_dataset:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - run: |
          python -m venv venv
          source venv/bin/activate

          python generate_dataset.py \
            --num_samples ${{ inputs.customers }} \
            --num_recommendations ${{ inputs.recommendations }} \
            --product_pool_size ${{ inputs.product_pool_size }}

          ls -lh uncommitted/

      - uses: actions/upload-artifact@v4
        with:
          name: data_sample
          path: |
            uncommitted/recommendations_dataset*

  measure_performance:
    needs: generate_dataset
    runs-on: ubuntu-slim # 5GB RAM
    strategy:
      matrix:
        script:
          - load_json.py
          - load_jsonl.py
          - load_pandas_json.py
          - load_pandas_jsonl.py
          - load_polars_json.py
          - load_polars_jsonl.py
          - load_interning_json.py
          - load_interning_jsonl.py
          - load_pickle.py
          - load_interned_pickle.py
          - load_dbm.py
          - load_shelve.py

    steps:
      - uses: actions/checkout@v3

      - uses: actions/download-artifact@v4
        with:
          name: data_sample
          path: uncommitted/

      - name: Run Analysis with vmstat monitoring
        continue-on-error: true
        run: |
          python -m venv venv
          source venv/bin/activate
          
          if [[ ${{ matrix.script }} =~ "pandas" ]]; then
            pip install 'pandas>=2.2.0'
          fi

          if [[ ${{ matrix.script }} =~ "polars" ]]; then
            pip install 'polars>=1.36.1'
          fi

          # Start vmstat in the background
          # -n suppresses repeating headers
          # timestamps, megabytes, don't repeat header, 1 is the delay in seconds
          mkdir -p uncommitted/outputs/vmstat
          vmstat -t -S M -n 1 > uncommitted/outputs/vmstat/vmstat_${{ matrix.script }}.log &
          VMSTAT_PID=$!
          echo "vmstat running with PID: $VMSTAT_PID"

          time python ${{ matrix.script }} || true

          # Stop vmstat
          kill $VMSTAT_PID

      - uses: actions/upload-artifact@v4
        with:
          name: perf-output-${{ matrix.script }}
          path: uncommitted/outputs/**/*.*

  build_image_and_push:
    runs-on: ubuntu-latest
    needs: generate_dataset
    
    permissions:
      contents: read
      packages: write 

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: data_sample
          path: uncommitted/

      - name: Log in to the Container registry
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest

  generate_summary:
    needs: measure_performance
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          # Match the unique pattern from the matrix job
          pattern: perf-output-*
          # Merge all matched artifacts into one directory
          merge-multiple: true
          path: raw_data/

      - name: Organize Artifacts
        run: |
          mkdir -p performance_data/vmstat
          
          find raw_data -name "vmstat_*.log" -exec mv {} performance_data/vmstat/ \;
          
          find raw_data -name "*.json" -print0 | while IFS= read -r -d '' file; do
              cat "$file"
              echo "" 
          done > performance_data/summary.jsonl

          echo "File structure prepared:"
          ls -R performance_data/

      - uses: actions/upload-artifact@v4
        with:
          name: summary
          path: performance_data